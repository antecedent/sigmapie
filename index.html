<!DOCTYPE html>
<html>

<head>
    <title>SigmaPie UI</title>
    <meta charset="utf-8">
    <script type="text/javascript"
        src="https://cdn.jsdelivr.net/npm/brython@3.8.7/brython.min.js">
    </script>
    <style type="text/css">
        * {
            margin: 0;
            padding: 0;
        }

        body {
            margin: 20px;
            font-size: 16px;
            font-family: 'Segoe UI', sans-serif;
        }

        #input textarea {
            display: block;
            height: 200px;
            width: calc(100vw - 50px);
            padding: 2px;
        }

        table {
            margin: 30px auto;
            border-collapse: collapse;
        }

        td:first-child {
            white-space: nowrap;
        }

        table, table tr, table th, table td {
            border: 1px solid gray;
        }
        
        table td, table th {
            padding: 3px 6px;
        }

        p, h1, h2 {
            margin: 30px auto;
            max-width: 800px;
            text-align: center;
        }

        #membership-field.Yes {
            background: #DFE;
        }

        #membership-field.No {
            background: #FDE;
        }

        #membership-result.Yes {
            color: #397;
        }

        #membership-result.No {
            color: #937;
        }

        button {
            padding: 4px;
        }

    </style>
</head>

<body onload="brython()">

    <h1>A web UI for SigmaPie, running on Brython</h1>

    <p><a href="https://github.com/alenaks/SigmaPie">SigmaPie</a> (created and maintained by Alëna Aksënova) is a library of grammatical inference algorithms for working "with subregular grammars both from the perspectives of theoretical linguistics and formal language theory".</p>

    <p><a href="https://brython.info/">Brython</a> is a Python 3 implementation for web browsers that makes it possible, among many other wonderful things, to run SigmaPie in this exact way: as a purely client-side web application.</p>

<section id="input">

    <h2>Input</h2>

    <p>Enter words separated by whitespace:</p>

    <textarea></textarea>

    

    <p>
    <button id="infer">Click to infer</button> a stringset from the 
    <select>
        <option value="2-MTSL" selected>2-MTSL</option>
        <option value="2-MTSL &amp; 3-SL">2-MTSL &amp; 3-SL</option>
        <option value="2-TSL">2-TSL</option>
        <option value="2-SP">2-SP</option>
        <option value="2-SL">2-SL</option>
        <option value="3-TSL">3-TSL</option>
        <option value="3-SP">3-SP</option>
        <option value="3-SL">3-SL</option>
        <option value="4-TSL">4-TSL</option>
        <option value="4-SP">4-SP</option>
        <option value="4-SL">4-SL</option>
    </select> class.
    </p>

    <p>
        You might also want to preprocess the input: click a button
        <button id="lowercase">to lowercase</button>, or
        <button id="remove-punctuation">to remove punctuation</button>.
    </p>

    <p>
        <input type="checkbox" id="syntax-mode"> Enable syntax mode (words treated as characters, lines as words)
    </p>
</section>

<section id="output">
</section>

<p>Using SigmaPie snapshot of 2 February 2020,<br>with minimal extra patching for a sub-asymptotic performance boost.</p>
<p>Made available as a web application by Ignas Rudaitis.</p>

<script type="text/python">

from browser import window, document, html

class FSM(object):
    """
    This class implements Finite State Machine.
    Attributes:
        initial (str): initial symbol;
        final (str): final symbol;
        transitions (list): triples of the form [prev_state,
            transition, next_state].
    """
    def __init__(self, initial, final, transitions=None):
        if transitions == None:
            self.transitions = []
        else:
            self.transitions = transitions

        self.initial = initial
        self.final = final


    def sl_to_fsm(self, grammar):
        """
        Creates FSM transitions based on the SL grammar.
        Arguments:
            grammar (list): SL ngrams.
        """
        if not grammar:
            raise ValueError("The grammar must not be empty.")
        self.transitions = [(i[:-1], i[-1], i[1:]) for i in grammar]


    def scan_sl(self, string):
        """
        Scans a given string using the learned SL grammar.
        Arguments:
            string (str): a string that needs to be scanned.
        Returns:
            bool: well-formedness value of the string.
        """
        if string[0] != self.initial or string[-1] != self.final:
            raise ValueError("The string is not annotated with "
                             "the delimeters.")
        if not self.transitions:
            raise ValueError("The transitions are empty. Extract the"
                             " transitions using grammar.fsmize().")

        k = len(self.transitions[0][0]) + 1
        for i in range(k - 1, len(string)):
            move_to_next = []
            for j in self.transitions:
                can_read = string[(i - k + 1):(i + 1)] == "".join(j[0]) + j[1]
                move_to_next.append(can_read)

            if not any(move_to_next):
                return False

        return True


    def trim_fsm(self):
        """
        This function trims useless transitions.
        1. Finds the initial state and collects the set of states to which one
           can come from that node and the nodes connected to it.
        2. Changes direction of the transitions and runs algorithm again to
           detect states from which one cannot get to the final state.
        As the result, self.transitions only contains useful transitions.
        """
        if not self.transitions:
            raise ValueError("Transtitions of the automaton must"
                             " not be emtpy.")
        can_start = self.accessible_states(self.initial)
        self.transitions = [(i[2], i[1], i[0]) for i in can_start]
        mirrored = self.accessible_states(self.final)
        self.transitions = [(i[2], i[1], i[0]) for i in mirrored]
        

    def accessible_states(self, marker):
        """
        Finds accessible states.
        Arguments:
            marker (str): initial or final state.
        Returns:
            list: list of transitions that can be made from
                the given initial or final state.
        """
        updated = self.transitions[:]
        
        # find initial/final transitions
        reachable = []
        for i in self.transitions:
            if i[0][0] == i[0][-1] == marker:
                reachable.append(i)
                updated.remove(i)

        # to keep copies that can be modified while looping
        mod_updated = updated[:]
        mod_reachable = []
        first_time = True

        # find transitions that can be reached
        while mod_reachable != [] or first_time:
            mod_reachable = []
            first_time = False
            for p in updated:
                for s in reachable:
                    if p[0] == s[2]:
                        mod_reachable.append(p)
                        mod_updated.remove(p)
            updated = mod_updated[:]
            reachable.extend(mod_reachable)

        return reachable


    def sp_build_template(self, path, alphabet, k):
        """
        Generates a template for the given k-SP path.
        Arguments:
            path (str): the sequence for which the template is generated;
            alphabet (list): list of all symbols of the grammar;
            k (int): window size of the grammar.
        """

        # creating the "sceleton" of the FSM
        for i in range(k-1):
            # boolean shows whether the transition was accessed
            self.transitions.append([i, path[i], i + 1, False])

        # adding non-final loops
        newtrans = []
        for t in self.transitions:
            for s in alphabet:
                if s != t[1]:
                    newtrans.append([t[0], s, t[0], False])

        # adding final loops
        for s in alphabet:
            newtrans.append([self.transitions[-1][2], s, self.transitions[-1][2], False])

        self.transitions += newtrans


    def sp_fill_template(self, sequence):
        """
        Runs the imput sequence through the SP automaton
        and marks transitions if they were taken. Cleans
        transitions that were not taken afterwards.
        Arguments:
            sequence (str): sequence of symbols that needs to be
                passed through the automaton.
        """
        state = 0
        for s in sequence:
            for t in self.transitions:
                if (t[0] == state) and (t[1] == s):
                    state = t[2]
                    t[3] = True
                    break


    def sp_clean_template(self):
        """ Removes transitions that were not accessed. """
        self.transitions = [i[:3] for i in self.transitions if i[3] == True]


    def scan_sp(self, string):
        """ Runs the given sequence through the automaton.
        Arguments:
            string (str): string to run through the automaton.
        Returns:
            bool: True if input can be accepted by the automaton,
                otherwise False.
        """
        state = 0
        for s in string:
            change = False
            for t in self.transitions:
                if (t[0] == state) and (t[1] == s):
                    state = t[2]
                    change = True
                    break

            if not change:
                return False

        return True

class FSMFamily(object):
    """
    This class encodes Family of Finite State Machines. Used for 
    a simple encoding of FSMs corresponding to SP languages.
    Attributes:
      transitions(list): triples of the form 
        [prev_state, transition, next_state].
    """

    def __init__(self, family=None):
        """ Initializes the FSMFamily object. """
        if family is None:
          self.family = []
        else:
          self.family = family


    def run_all_fsm(self, string):
        """
        Tells whether the given string is accepted by all
        the automata of the family.
        Arguments:
            string (str): the input string.
        Returns:
            bool: True if the string is accepted by all the
                fsms, otherwise False.
        """
        return all([f.scan_sp(string) for f in self.family])
        

def deepcopy(x, memo=None, _nil=[]):
    """Deep copy operation on arbitrary Python objects.
    See the module's __doc__ string for more info.
    """

    if memo is None:
        memo = {}

    d = id(x)
    y = memo.get(d, _nil)
    if y is not _nil:
        return y

    cls = type(x)

    copier = _deepcopy_dispatch.get(cls)
    if copier is not None:
        y = copier(x, memo)
    else:
        if issubclass(cls, type):
            y = _deepcopy_atomic(x, memo)
        else:
            copier = getattr(x, "__deepcopy__", None)
            if copier is not None:
                y = copier(memo)
            else:
                reductor = dispatch_table.get(cls)
                if reductor:
                    rv = reductor(x)
                else:
                    reductor = getattr(x, "__reduce_ex__", None)
                    if reductor is not None:
                        rv = reductor(4)
                    else:
                        reductor = getattr(x, "__reduce__", None)
                        if reductor:
                            rv = reductor()
                        else:
                            raise Error(
                                "un(deep)copyable object of type %s" % cls)
                if isinstance(rv, str):
                    y = x
                else:
                    y = _reconstruct(x, memo, *rv)

    # If is its own copy, don't memoize.
    if y is not x:
        memo[d] = y
        _keep_alive(x, memo) # Make sure x lives at least as long as d
    return y

_deepcopy_dispatch = d = {}

def _deepcopy_atomic(x, memo):
    return x
d[type(None)] = _deepcopy_atomic
d[type(Ellipsis)] = _deepcopy_atomic
d[type(NotImplemented)] = _deepcopy_atomic
d[int] = _deepcopy_atomic
d[float] = _deepcopy_atomic
d[bool] = _deepcopy_atomic
d[complex] = _deepcopy_atomic
d[bytes] = _deepcopy_atomic
d[str] = _deepcopy_atomic
d[type] = _deepcopy_atomic


def _deepcopy_list(x, memo, deepcopy=deepcopy):
    y = []
    memo[id(x)] = y
    append = y.append
    for a in x:
        append(deepcopy(a, memo))
    return y
d[list] = _deepcopy_list

def _deepcopy_tuple(x, memo, deepcopy=deepcopy):
    y = [deepcopy(a, memo) for a in x]
    # We're not going to put the tuple in the memo, but it's still important we
    # check for it, in case the tuple contains recursive mutable structures.
    try:
        return memo[id(x)]
    except KeyError:
        pass
    for k, j in zip(x, y):
        if k is not j:
            y = tuple(y)
            break
    else:
        y = x
    return y
d[tuple] = _deepcopy_tuple

def _deepcopy_dict(x, memo, deepcopy=deepcopy):
    y = {}
    memo[id(x)] = y
    for key, value in x.items():
        y[deepcopy(key, memo)] = deepcopy(value, memo)
    return y
d[dict] = _deepcopy_dict


def _deepcopy_method(x, memo): # Copy instance methods
    return type(x)(x.__func__, deepcopy(x.__self__, memo))

del d

class FST():
    """
    A class representing finite state transducers.
    Attributes:
        Q (list): a list of states;
        Sigma (list): a list of symbols of the input alphabet;
        Gamma (list): a list of symbols of the output alphabet;
        qe (str): name of the unique initial state;
        E (list): a list of transitions;
        stout (dict): a collection of state outputs.
    """
    def __init__(self, Sigma=None, Gamma=None):
        """ Initializes the FST object. """
        self.Q = None
        self.Sigma = Sigma
        self.Gamma = Gamma
        self.qe = ""
        self.E = None
        self.stout = None


    def rewrite(self, w):
        """
        Rewrites the given string with respect to the rules represented
        in the current FST.
        Arguments:
            w (str): a string that needs to be rewritten.
        Outputs:
            str: the translation of the input string.
        """
        if self.Q == None:
            raise ValueError("The transducer needs to be constructed.")
        
        # move through the transducer and write the output
        result = ""
        current_state = ""
        moved = False
        for i in range(len(w)):
            for tr in self.E:
                if tr[0] == current_state and tr[1] == w[i]:
                    result += tr[2]
                    current_state, moved = tr[3], True
                    break
            if moved == False:
                raise ValueError("This string cannot be read by the current transducer.")
                
        # add the final state output
        if self.stout[current_state] != "*":
            result += self.stout[current_state]
            
        return result
        
        
        
    def copy_fst(self):
        """
        Produces a deep copy of the current FST.
        Returns:
            T (FST): a copy of the current FST.
        """
        T = FST()
        T.Q = deepcopy(self.Q)
        T.Sigma = deepcopy(self.Sigma)
        T.Gamma = deepcopy(self.Gamma)
        T.E = deepcopy(self.E)
        T.stout = deepcopy(self.stout)
        
        return T
        

class L(object):
    """
    A general class for grammars and languages. Implements methods that
    are applicable to all grammars in this package.
    Attributes:
        alphabet (list): alphabet used in the language;
        grammar (list): the list of substructures;
        k (int): locality window;
        data (list): input data;
        edges (list): start- and end-symbols for the grammar;
        polar ("p" or "n"): polarity of the grammar.
    """
    def __init__(self, alphabet=None, grammar=None, k=2, data=None,
                 edges=[">", "<"], polar="p"):
        """ Initializes the L object. """
        if polar not in ["p", "n"]:
            raise ValueError("The value of polarity should be either "
                             "positive ('p') or negative ('n').")
        self.__polarity = polar
        self.alphabet = alphabet
        self.grammar = [] if grammar is None else grammar
        self.k = k
        self.data = [] if data is None else data
        self.edges = edges


    def extract_alphabet(self):
        """
        Extracts alphabet from the given data or grammar and saves it
        into the 'alphabet' attribute.
        CAUTION: if not all symbols were used in the data or grammar,
                the result is not correct: update manually.
        """
        if self.alphabet is None:
            self.alphabet = []
        symbols = set(self.alphabet)
        if self.data:
            for item in self.data:
                symbols.update({j for j in item})
        if self.grammar:
            for item in self.grammar:
                symbols.update({j for j in item})
        symbols = symbols - set(self.edges)
        self.alphabet = sorted(list(symbols))


    def well_formed_ngram(self, ngram):
        """
        Tells if the given ngram is well-formed.
        An ngram is ill-formed if:
        * there is something in-between two start- or end-symbols
          ('>a>'), or
        * something is before start symbol or after the end symbol
          ('a>'), or
        * the ngram consists only of start- or end-symbols.
        Otherwise it is well-formed.
        Arguments:
            ngram (str): The ngram that needs to be evaluated.
        Returns:
            bool: well-formedness of the ngram.
        """
        start, end = [], []
        for i in range(len(ngram)):
            if ngram[i] == self.edges[0]:
                start.append(i)
            elif ngram[i] == self.edges[1]:
                end.append(i)

        start_len, end_len = len(start), len(end)
        if any([start_len == len(ngram), end_len == len(ngram)]):
            return False

        if start_len > 0:
            if ngram[0] != self.edges[0]:
                return False
            if start_len > 1:
                for i in range(1, start_len):
                    if start[i] - start[i-1] != 1:
                        return False

        if end_len > 0:
            if ngram[-1] != self.edges[1]:
                return False
            if end_len > 1:
                for i in range(1, end_len):
                    if end[i] - end[i-1] != 1:
                        return False

        return True


    def generate_all_ngrams(self, symbols, k):
        """
        Generates all possible ngrams of the length k based on the
        given alphabet.
        Arguments:
            alphabet (list): alphabet;
            k (int): locality window (length of ngram).
        Returns:
            list: generated ngrams.
        """
        symb = symbols[:]
        if not ((self.edges[0] in symb) or (self.edges[1] in symb)):
            symb += self.edges

        combinations = product(symb, repeat=k)
        ngrams = []
        for ngram in combinations:
            if self.well_formed_ngram(ngram) and (ngram not in ngrams):
                ngrams.append(ngram)

        return ngrams


    def opposite_polarity(self, symbols):
        """
        Returns the grammar opposite to the one given.
        Arguments:
            symbols (list): alphabet.
        Returns:
            list: ngrams of the opposite polarity.
        """
        all_ngrams = self.generate_all_ngrams(symbols, self.k)
        opposite = [i for i in all_ngrams if i not in self.grammar]

        return opposite


    def check_polarity(self):
        """ Returns the polarity of the grammar ("p" or "n"). """
        if self.__polarity == "p":
            return "p"
        return "n"


    def change_polarity(self, new_polarity=None):
        """
        Changes the polarity of the grammar.
        Warning: it does not rewrite the grammar!
        """
        if new_polarity is not None:
            if new_polarity not in ["p", "n"]:
                raise ValueError("The value of polarity should be either "
                                "positive ('p') or negative ('n').")
            self.__polarity = new_polarity
        else:
            if self.__polarity == "p":
                self.__polarity = "n"
            elif self.__polarity == "n":
                self.__polarity = "p"
                

def alphabetize(data):
    """
    Detects symbols used in the input data.
    Arguments:
        data (list): Input data.
    Returns:
        list:  Symbols used in these examples.
    """
    alphabet = set()
    for item in data:
        alphabet.update({i for i in item})
    return sorted(list(alphabet))


def get_gram_info(ngrams):
    """
    Returns the alphabet and window size of the grammar.
    Arguments:
        ngrams (list): list of ngrams.
    Returns:
        (list, int)
            list: alphabet;
            int: locality window.
    """
    alphabet = list(set([i for i in "".join(ngrams) if i not in [">", "<"]]))
    k = max(len(i) for i in ngrams)
    return alphabet, k


def prefix(w):
    """
    Returns a list of prefixes of a given string.
    Arguments:
        w (str): a string prefixes of which need to be extracted.
    Returns:
        list: a list of prefixes of the given string.
    """
    return [w[:i] for i in range(len(w)+1)]



def lcp(*string):
    """
    Finds the longest common prefix of an unbounded number of strings.
    Arguments:
        *string (str): one or more strings;
    Returns:
        str: a longest common prefix of the input strings.
    """
    w = list(set(i for i in string if i != "*"))
    if not w:
        raise IndexError("At least one non-unknown string needs to be provided.")
    
    result = ""
    n = min([len(x) for x in w])
    for i in range(n):
        if len(set(x[i] for x in w)) == 1:
            result += w[0][i]
        else:
            break
    
    return result



def remove_from_prefix(w, pref):
    """
    Removes a substring from the prefix position of another string.
    Arguments:
        w (str): a string that needs to be modified;
        pref (str): a prefix that needs to be removed from the string.
    Returns:
        str: the modified string.
    """
    if w.startswith(pref):
        return w[len(pref):]
    elif w == "*":
        return w

    raise ValueError(pref + " is not a prefix of " + w)
    


def randint(a, b):
    return int(window.Math.random()*(b-a+1)+a)

def choice(pool):
    k = int(window.Math.random() * len(pool))
    return pool[k]

def product(*args, repeat=1):
    # product('ABCD', 'xy') --> Ax Ay Bx By Cx Cy Dx Dy
    # product(range(2), repeat=3) --> 000 001 010 011 100 101 110 111
    pools = [tuple(pool) for pool in args] * repeat
    result = [[]]
    for pool in pools:
        result = [x+[y] for x in result for y in pool]
    for prod in result:
        yield tuple(prod)


class SP(L):
    """
    A class for strictly piecewise grammars and languages.
    Attributes:
        alphabet (list): alphabet used in the language;
        grammar (list): collection of ngrams;
        k (int): locality window;
        data (list): input data;
        polar ("p" or "n"): polarity of the grammar;
        fsm (FSM): corresponding finite state machine.
    """
    def __init__(self, alphabet=None, grammar=None, k=2, data=None,
                 polar="p"):
        """ Initializes the SP object. """
        super().__init__(alphabet, grammar, k, data, polar=polar)
        self.fsm = FSMFamily()


    def subsequences(self, string):
        """
        Extracts k-long subsequences out of the given word.
        Arguments:
            string (str): a string that needs to be processed.
        Returns:
            list: a list of subsequences out of the string.
        """
        if len(string) < self.k:
            return []
        
        start = list(string[:self.k])
        result = [start]

        previous_state = [start]
        current_state = []

        for s in string[self.k:]:
            for p in previous_state:
                for i in range(self.k):
                    new = p[:i] + p[i+1:] + [s]
                    if new not in current_state:
                        current_state.append(new)
            result.extend(current_state)
            previous_state = current_state[:]
            current_state = []
            
        return list(set([tuple(i) for i in result]))


    def learn(self):
        """
        Extracts k-long subsequences from the training data.
        Results:
            self.grammar is updated.
        """
        if not self.data:
            raise ValueError("The data must be provided.")
        if not self.alphabet:
            raise ValueError("The alphabet must be provided. To "
                             "extract the alphabet automatically, "
                             "run `grammar.extract_alphabet()`.")

        self.grammar = []
        for i in self.data:
            for j in self.subsequences(i):
                if j not in self.grammar:
                    self.grammar.append(j)

        if self.check_polarity() == "n":
            self.grammar = self.opposite_polarity()


    def opposite_polarity(self):
        """ Returns the grammar opposite to the current one. """
        all_ngrams = product(self.alphabet, repeat = self.k)
        return [i for i in all_ngrams if i not in self.grammar]


    def fsmize(self):
        """
        Creates FSM family for the given SP grammar by passing every
        encountered subsequence through the corresponding automaton.
        """
        if not self.grammar:
            self.learn()

        if self.check_polarity() == "p":
            data_subseq = self.grammar[:]
        else:
            data_subseq = self.opposite_polarity()

        # create a family of templates in fsm attribute
        seq = product(self.alphabet, repeat = self.k-1)
        for path in seq:
            f = FSM(initial=None, final=None)
            f.sp_build_template(path, self.alphabet, self.k)
            self.fsm.family.append(f)

        # run the input/grammar through the fsm family
        for f in self.fsm.family:
            for r in data_subseq:
                f.sp_fill_template(r)

        # clean the untouched transitions
        for f in self.fsm.family:
            f.sp_clean_template()


    def scan(self, string):
        """
        Tells if the input string is well-formed.
        Arguments:
            string (str): string to be scanned.
        Returns:
            bool: True is well-formed, otherwise False.
        """
        subseq = self.subsequences(string)
        found_in_G = [(s in self.grammar) for s in subseq]
        
        if self.check_polarity == "p":
            return all(found_in_G)
        else:
            return not any(found_in_G)


    def generate_item(self):
        """
        Generates a well-formed string.
        Returns:
            str: the generated string.
        """
        if not self.alphabet:
            raise ValueError("The alphabet must be provided.")

        string = ""
        while True:
            options = []
            for i in self.alphabet:
                if self.scan(string + i):
                    options.append(i)

            add = choice(options + ["EOS"])
            if add == "EOS":
                return string
            else:
                string += add


    def generate_sample(self, n=10, repeat=False, safe=True):
        """
        Generates data sample of desired length.
        Arguments:
            n (int): the number of examples to be generated,
                the default value is 10;
            repeat (bool): allow (rep=True) or prohibit (rep=False)
               repetitions, the default value is False;
            safe (bool): automatically break out of infinite loops,
                for example, when the grammar cannot generate the
                required number of data items, and the repetitions
                are set to False.
        Returns:
            list: a list of generated examples.
        """
        sample = [self.generate_item() for i in range(n)]

        if not repeat:
            useless_loops = 0
            sample = set(sample)
            prev_len = len(sample)

            while len(list(set(sample))) < n:
                sample.add(self.generate_item())
                if prev_len == len(sample):
                    useless_loops += 1
                else:
                    useless_loops = 0

                if safe and useless_loops > 100:
                    print("The grammar cannot produce the requested number"
                          " of strings.")
                    break

        return list(sample)


    def switch_polarity(self, new_polarity=None):
        """
        Changes the polarity of the grammar.
        Arguments:
            new_polarity ("p" or "n"): the new value of the polarity.
        """
        old_value = self.check_polarity()
        self.change_polarity(new_polarity)
        new_value = self.check_polarity()

        if old_value != new_value:
            self.grammar = self.opposite_polarity()



    def clean_grammar(self):
        """
        Removes useless ngrams from the grammar.
        If negative, it just removes duplicates.
        If positive, it detects bigrams to which one cannot get
            from the initial symbol and from which one cannot get
            to the final symbol, and removes them.
        """
        self.grammar = list(set(self.grammar))
        


class SL(L):
    """
    A class for strictly local grammars and languages.
    Attributes:
        alphabet (list): alphabet used in the language;
        grammar (list): collection of ngrams;
        k (int): locality window;
        data (list): input data;
        edges (list): start- and end-symbols for the grammar;
        polar ("p" or "n"): polarity of the grammar;
        fsm (FSM): corresponding finite state machine.
    """
    def __init__(self, alphabet=None, grammar=None, k=2, data=None,
                 edges=[">", "<"], polar="p"):
        """ Initializes the SL object. """
        super().__init__(alphabet, grammar, k, data, edges, polar)
        self.fsm = FSM(initial=self.edges[0], final=self.edges[1])


    def learn(self):
        """ Extracts SL grammar from the given data. """
        self.grammar = self.ngramize_data()
        if self.check_polarity() == "n":
            self.grammar = self.opposite_polarity(self.alphabet)

                
    def annotate_string(self, string):
        """ Annotates the string with the start and end symbols.
        Arguments:
            string (str): a string that needs to be annotated.
        Returns:
            str: annotated version of the string.
        """
        return ">" * (self.k - 1) + string.strip() + "<" * (self.k - 1)
        

    def ngramize_data(self):
        """
        Creates set of n-grams based on the given data.
        Returns:
            list: collection of ngrams in the data.
        """
        if not self.data:
            raise ValueError("The data is not provided.")

        ngrams = []
        for s in self.data:
            item = self.annotate_string(s)
            ngrams.extend(self.ngramize_item(item))

        return list(set(ngrams))


    def ngramize_item(self, item):
        """
        This function n-gramizes a given string.
        Arguments:
            item (str): a string that needs to be ngramized.
        Returns:
            list: list of ngrams from the item.
        """
        ng = []
        for i in range(len(item) - (self.k - 1)):
            ng.append(tuple(item[i:(i + self.k)]))

        return list(set(ng))


    def fsmize(self):
        """
        Builds FSM corresponding to the given grammar and saves it
        in the fsm attribute.
        """
        if not self.grammar:
            raise(IndexError("The grammar must not be empty."))
        if not self.alphabet:
            raise ValueError("The alphabet is not provided. "
                             "Use `grammar.extract_alphabet()`.")

        if self.check_polarity() == "p":
            self.fsm.sl_to_fsm(self.grammar)
        else:
            opposite = self.opposite_polarity(self.alphabet)
            self.fsm.sl_to_fsm(opposite)


    def scan(self, string):
        """
        Checks if the given string is well-formed with respect
        to the given grammar.
        Arguments:
            string (str): the string that needs to be evaluated.
        Returns:
            bool: well-formedness value of a string.
        """
        if not self.fsm.transitions:
            self.fsmize()
            
        string = self.annotate_string(string)
        return self.fsm.scan_sl(string)


    def generate_sample(self, n=10, repeat=True, safe=True):
        """
        Generates a data sample of the required size, with or without
        repetitions depending on `repeat` value.
        Arguments:
            n (int): the number of examples to be generated;
            repeat (bool): allows (rep=True) or prohibits (rep=False)
               repetitions within the list of generated items;
            safe (bool): automatically breaks out of infinite loops,
                for example, when the grammar cannot generate the
                required number of data items, and the repetitions
                are set to False.
        Returns:
            list: generated data sample.
        """
        if not self.alphabet:
            raise ValueError("Alphabet cannot be empty.")
        if not self.fsm.transitions:
            self.fsmize()

        statemap = self.state_map()
        if not any([len(statemap[x]) for x in statemap]):
            raise(ValueError("There are ngrams in the grammar that are"
                            " not leading anywhere. Clean the grammar "
                            " or run `grammar.clean_grammar()`."))

        data = [self.generate_item(statemap) for i in range(n)]

        if not repeat:
            data = set(data)
            useless_loops = 0
            prev_len = len(data)

            while len(data) < n:
                data.add(self.generate_item(statemap))

                if prev_len == len(data):
                    useless_loops += 1
                else:
                    useless_loops = 0
                
                if safe and useless_loops > 500:
                    print("The grammar cannot produce the requested "
                          "number of strings. Check the grammar, "
                          "reduce the number, or allow repetitions.")
                    break

        return list(data)

                
    def generate_item(self, statemap):
        """
        Generates a well-formed string with respect to the given grammar.
        Arguments:
            statemap (dict): a dictionary of possible transitions in the 
                corresponding fsm; constructed inside generate_sample.
        Returns:
            str: a well-formed string.
        """
        word = self.edges[0] * (self.k - 1)
        while word[-1] != self.edges[1]:
            word += choice(statemap[word[-(self.k - 1):]])
        return word[(self.k - 1):-1]


    def state_map(self):
        """
        Generates a dictionary of possible transitions in the FSM.
        Returns:
            dict: the dictionary of the form
                {"keys":[list of possible next symbols]}, where 
                keys are (k-1)-long strings.
        """
        local_alphabet = self.alphabet[:] + self.edges[:]
        poss = product(local_alphabet, repeat=(self.k - 1))
        
        smap = {}
        for i in poss:
            for j in self.fsm.transitions:
                if j[0] == i:
                    before = "".join(i)
                    if before in smap:
                        smap[before] += j[1]
                    else:
                        smap[before] = [j[1]]
        return smap


    def switch_polarity(self):
        """
        Changes polarity of the grammar, and changes the grammar
        to the opposite one.
        """
        if not self.alphabet:
            raise ValueError("Alphabet cannot be empty.")

        self.grammar = self.opposite_polarity(self.alphabet)
        self.change_polarity()


    def clean_grammar(self):
        """
        Removes useless ngrams from the grammar.
        If negative, it just removes duplicates.
        If positive, it detects bigrams to which one cannot get
            from the initial symbol and from which one cannot get
            to the final symbol, and removes them.
        """
        if not self.fsm.transitions:
            self.fsmize()
            
        if self.check_polarity() == "n":
            self.grammar = list(set(self.grammar))
        else:
            self.fsm.trim_fsm()
            self.grammar = [j[0] + (j[1],) for j in self.fsm.transitions]



class TSL(SL):
    """
    A class for tier-based strictly local grammars and languages.
    Attributes:
        alphabet (list): alphabet used in the language;
        grammar (list): the list of substructures;
        k (int): locality window;
        data (list): input data;
        edges (list): start- and end-symbols for the grammar;
        polar ("p" or "n"): polarity of the grammar;
        fsm (FSM): finite state machine that corresponds to the grammar;
        tier (list): list of tier symbols.
    """
    def __init__(self, alphabet=None, grammar=None, k=2, data=None,
                 edges=[">", "<"], polar="p", tier=None):
        """ Initializes the TSL object. """
        super().__init__(alphabet, grammar, k, data, edges, polar)
        self.tier = tier
        self.fsm = FSM(initial=self.edges[0], final=self.edges[1])


    def learn(self):
        """ 
        Learns tier and finds attested (if positive) or unattested
        (if negative) ngrams of the tier images of the data. 
        """
        if not self.alphabet:
            raise ValueError("Alphabet cannot be empty.")
        if not self.data:
            raise ValueError("Data needs to be provided.")

        self.learn_tier()
        tier_sequences = [self.tier_image(i) for i in self.data]
        self.grammar = TSL(k=self.k, data=tier_sequences).ngramize_data()

        if self.check_polarity() == "n":
            self.grammar = self.opposite_polarity(self.tier)
            

    def learn_tier(self):
        """
        This function determines which of the symbols used in
        the language are tier symbols, algorithm by Jardine &
        McMullin (2017). Updates tier attribute.
        """
        self.tier = self.alphabet[:]
        ngrams = self.ngramize_data()

        ngrams_less = TSL(data=self.data, k=(self.k - 1)).ngramize_data()
        ngrams_more = TSL(data=self.data, k=(self.k + 1)).ngramize_data()

        for symbol in self.tier:
            if self.test_insert(symbol, ngrams, ngrams_less) and \
               self.test_remove(symbol, ngrams, ngrams_more):
                self.tier.remove(symbol)


    def test_insert(self, symbol, ngrams, ngrams_less):
        """ 
        Tier presense test #1. For every (n-1)-gram ('x','y','z'),
        there must be n-grams of the type ('x','S','y','z') and
        ('x','y','S','z').
        Arguments:
            symbol (str): the symbol that is currently being tested;
            ngrams (list): the list of n-gramized input;
            ngrams_less (list): the list of (n-1)-gramized input.
        Returns:
            bool: True if a symbol passed the test, otherwise False.
        """
        extension = []
        for small in ngrams_less:
            for i in range(len(small)):
                new = small[:i] + (symbol,) + small[i:]
                if self.well_formed_ngram(new):
                    extension.append(new)

        return set(extension).issubset(set(ngrams))


    def test_remove(self, symbol, ngrams, ngrams_more):
        """
        Tier presense test #2. For every (n+1)-gram of the type 
        ('x','S','y'), there must be an n-gram of the type ('x', 'y').
        Arguments:
            symbol (str): the symbol that is currently being tested;
            ngrams (list): the list of n-gramized input;
            ngrams_more (list): the list of (n+1)-gramized input.
        Returns:
            bool: True if a symbol passed the test, otherwise False.
        """
        extension = []
        for big in ngrams_more:
            if symbol in big:
                for i in range(len(big)):
                    if big[i] == symbol:
                        new = big[:i] + big[i+1:]
                        if self.well_formed_ngram(new):
                            extension.append(new)

        return set(extension).issubset(set(ngrams))


    def tier_image(self, string):
        """
        Function that returns a tier image of the input string.
        Arguments:
            string (str): string that needs to be processed.
        Returns:
            str: tier image of the input string.
        """
        return "".join(i for i in string if i in self.tier)


    def fsmize(self):
        """
        Builds FSM corresponding to the given grammar and saves in
        it the fsm attribute.
        """
        if not self.grammar:
            raise(IndexError("The grammar must not be empty."))
        if not self.tier:
            raise ValueError("The tier is not extracted or empty. "
                             "Switch to SL or use `grammar.learn()`.")

        if self.check_polarity() == "p":
            self.fsm.sl_to_fsm(self.grammar)
        else:
            opposite = self.opposite_polarity(self.tier)
            self.fsm.sl_to_fsm(opposite)


    def switch_polarity(self):
        """
        Changes polarity of the grammar, and rewrites
        grammar to the opposite one.
        """
        if not self.tier:
            raise ValueError("Either the language is SL, or the tier "
                             "is not extracted, use `grammar.learn()`.")

        self.grammar = self.opposite_polarity(self.tier)
        self.change_polarity()
        

    def generate_sample(self, n=10, repeat=True, safe=True):
        """
        Generates n well-formed strings, with or without repetitions.
        Arguments:
            n (int): the number of examples to be generated;
            repeat (bool): allow (rep=True) or prohibit (rep=False)
               repetitions of the same data items;
            safe (bool): automatically break out of infinite loops,
                for example, when the grammar cannot generate the
                required number of data items, and the repetitions
                are set to False.
        Returns:
            list: generated data sample.
        """
        if not self.alphabet:
            raise ValueError("Alphabet cannot be empty.")
        if not self.tier:
            raise ValueError("Either the language is SL, or the tier "
                             "is not extracted, use `grammar.learn()`.")

        if len(self.alphabet) == len(self.tier):
            sl = SL(polar = self.check_polarity())
            sl.alphabet = self.alphabet
            sl.grammar = self.grammar
            sl.k = self.k
            sl.edges = self.edges
            sl.fsmize()
            return sl.generate_sample(n, repeat, safe)

        if not self.fsm.transitions:
            self.fsmize()

        statemap = self.state_map()
        data = [self.generate_item() for i in range(n)]

        if not repeat:
            data = set(data)
            useless_loops = 0
            prev_len = len(data)
            while len(data) < n:
                data.add(self.generate_item())
                if prev_len == len(data): useless_loops += 1
                else: useless_loops = 0
                
                if safe and useless_loops > 100:
                    print("The grammar cannot produce the requested number"
                          " of strings.")
                    break

        return list(data)


    def generate_item(self):
        """
        Generates a well-formed sequence of symbols.
        Returns:
            str: a well-formed string.
        """
        if not self.fsm.transitions:
            self.fsmize()

        statemap = self.state_map()
        if not any([len(statemap[x]) for x in statemap]):
            raise(ValueError("There are ngrams in the grammar that are"
                            " not leading anywhere. Clean the grammar "
                            " or run `grammar.clean_grammar()`."))

        tier_seq = self.annotate_string(super().generate_item(statemap))
        ind = [x for x in range(len(tier_seq)) if tier_seq[x] not in self.edges]
        if not ind:
            tier_items = []
        else:
            tier_items = list(tier_seq[ind[0]:(ind[-1] + 1)])

        free_symb = list(set(self.alphabet).difference(set(self.tier)))
        
        new_string = self.edges[0] * (self.k - 1)
        for i in range(self.k + 1):
            if randint(0, 1) and free_symb:
                new_string += choice(free_symb)

        if not tier_items:
            return "".join([i for i in new_string if i not in self.edges])

        for item in tier_items:
            new_string += item
            for i in range(self.k + 1):
                if randint(0, 1) and free_symb:
                    new_string += choice(free_symb)

        return "".join([i for i in new_string if i not in self.edges])


    def state_map(self):
        """
        Generates a dictionary of possible transitions in the FSM.
        Returns:
            dict: the dictionary of the form
                {"keys":[list of possible next symbols]}, where 
                keys are (k-1)-long strings.
        """
        if self.fsm is None:
            self.fsmize()

        local_alphabet = self.tier[:] + self.edges[:]
        poss = product(local_alphabet, repeat=(self.k - 1))

        smap = {}
        for i in poss:
            for j in self.fsm.transitions:
                if j[0] == i:
                    before = "".join(i)
                    if before in smap:
                        smap[before] += j[1]
                    else:
                        smap[before] = [j[1]]
        return smap


    def scan(self, string):
        """
        Checks if the given string is well-formed with respect
        to the given grammar.
        Arguments:
            string (str): the string that needs to be evaluated.
        Returns:
            bool: well-formedness value of a string.
        """
        tier_img = self.annotate_string(self.tier_image(string))
        matches = [(n in self.grammar) for n in self.ngramize_item(tier_img)]

        if self.check_polarity() == "p":
            return all(matches)
        else:
            return not any(matches)

        


class MTSL(TSL):
    """
    A class for tier-based strictly local grammars and languages.
    Attributes:
        alphabet (list): alphabet used in the language;
        grammar (list): the list of substructures;
        k (int): locality window;
        data (list): input data;
        edges (list): start- and end-symbols for the grammar;
        polar ("p" or "n"): polarity of the grammar;
        fsm (FSMFamily): a list of finite state machines that 
            corresponds to the grammar;
        tier (list): list of tuples, where every tuple lists elements
            of some tier.
    Learning for k > 2 is not implemented: requires more theoretical work.
    """
    def __init__(self, alphabet=None, grammar=None, k=2, data=None,
                 edges=[">", "<"], polar="p"):
        """ Initializes the TSL object. """
        super().__init__(alphabet, grammar, k, data, edges, polar)
        self.fsm = FSMFamily()
        if self.k != 2:
            raise NotImplementedError("The learner for k-MTSL languages is "
                                      "still being designed.")
        self.tier = None
        self.tier_smap = None
        self.main_smap = None


    def learn(self):
        """
        Learns 2-local MTSL grammar for a given sample. The algorithm 
        currently works only for k=2 and is based on MTSL2IA designed 
        by McMullin, Aksenova and De Santo (2019). We are currently
        working on lifting the locality of the grammar to arbitrary k.
        Results:
            self.grammar is updated with a grammar of the following shape:
            {(tier_1):[bigrams_for_tier_1],
                ...
             (tier_n):[bigrams_for_tier_n]}
        """
        if not self.data:
            raise ValueError("Data needs to be provided.")
        if not self.alphabet:
            raise ValueError("The alphabet is empty. Provide data or "
                "run `grammar.extract_alphabet`.")

        possible = set(self.generate_all_ngrams(self.alphabet, self.k))
        attested = set()
        for d in self.data:
            bigrams = self.ngramize_item(self.annotate_string(d))
            attested.update(set(bigrams))
        unattested = list(possible.difference(attested))

        paths = self.all_paths(self.data)
        grammar = []

        for bgr in unattested:
            tier = self.alphabet[:]

            for s in self.alphabet:
                rmv = True

                # condition 1
                if s in bgr:
                    rmv = False
                    continue

                # condition 2
                relevant_paths = []
                for p in paths:
                    if (p[0] == bgr[0]) and (p[-1] == bgr[-1]) and (s in p[1]):
                        relevant_paths.append(p)
                for rp in relevant_paths:
                    new = (rp[0], tuple(i for i in rp[1] if i != s), rp[2])
                    if new not in paths:
                        rmv = False
                        break

                # remove from the tier if passed both conditions
                if rmv:
                    tier.remove(s)

            grammar.append((tier, bgr))
        gathered = self.gather_grammars(grammar)

        self.grammar = gathered
        self.tier = [i for i in self.grammar]

        if self.check_polarity() == "p":
            self.grammar = self.opposite_polarity()

        self.tier_smap = None
        self.main_smap = None


    def scan(self, string):
        """
        Scan string with respect to a given MTSL grammar.
        Arguments:
            string (str): a string that needs to be scanned.
        Returns:
            bool: well-formedness of the string.
        """
        tier_evals = []

        for tier in self.grammar:
            t = tier
            g = self.grammar[tier]

            delete_non_tier = "".join([i for i in string if i in t])
            tier_image = self.annotate_string(delete_non_tier)
            ngrams = self.ngramize_item((tier_image))

            this_tier = [(ngr in g) for ngr in ngrams]

            if self.check_polarity() == "p":
                tier_evals.append(all(this_tier))
            else:
                tier_evals.append(not any(this_tier))

        return all(tier_evals)


    def gather_grammars(self, grammar):
        """
        Gathers grammars with the same tier together.
        Arguments:
            grammar (list): a representation of the learned grammar
                where there is a one-to-one mapping between tiers 
                and bigrams.
        Returns:
            dict: a dictionary where keys are tiers and values are
                the restrictions imposed on those tiers.
        """
        G = {}
        for i in grammar:
            if tuple(i[0]) in G:
                G[tuple(i[0])] += [i[1]]
            else:
                G[tuple(i[0])] = [i[1]]
        return G


    def path(self, string):
        """
        Collects a list of paths from a string. A path is a 
        triplet <a, X, b>, where `a` is a symbol, `b` is a symbol
        that follows `a` in `string`, and `X` is a set of symbols 
        in-between `a` and `b`.
        Arguments:
            string (str): a string paths of which need to be found.
        Returns:
            list: list of paths of `string`.
        """
        string = self.annotate_string(string)
        paths = set()

        for i in range(len(string) - 1):
            for j in range(i + 1, len(string)):
                path = (string[i], tuple(set(k for k in string[(i + 1):j])), string[j])
                if path not in paths:
                    paths.add(path)

        return paths


    def all_paths(self, dataset):
        """
        Finds all paths that are present in a list of strings.
        Arguments:
            dataset (list): a list of strings.
        Returns:
            list: a list of paths present in `dataset`.
        """
        paths = set()
        for item in dataset:
            paths |= self.path(item)
        return paths


    def opposite_polarity(self):
        """
        Generates a grammar of the opposite polarity.
        Returns:
            dict: a dictionary containing the opposite ngram lists
                for every tier of the grammar.
        """
        if not self.grammar:
            raise ValueError("Grammar needs to be provided. It can also "
                             "be learned using `grammar.learn()`.")
        opposite = {}
        for i in self.grammar:
            possib = self.generate_all_ngrams(list(i), self.k)
            opposite[i] = [j for j in possib if j not in self.grammar[i]]

        return opposite


    def switch_polarity(self):
        """ Changes polarity of the grammar, and rewrites
            grammar to the opposite one.
        """
        self.grammar = self.opposite_polarity()
        self.change_polarity()


    def map_restrictions_to_fsms(self):
        """
        Maps restrictions to FSMs: based on the grammar, it creates
        a list of lists, where every sub-list has the following shape:
        [tier_n, restrictions_n, fsm_n]. Such sub-list is constructed
        for every single tier of the current MTSL grammar.
        Returns:
            [list, list, FSM]
                list: a list of current tier's symbols;
                list: a list of current tier's restrictions;
                FSM: a FSM corresponding to the current tier.
        """
        if not self.grammar:
            raise(IndexError("The grammar must not be empty."))

        restr_to_fsm = []

        for alpha, ngrams in self.grammar.items():
            polarity = self.check_polarity()
            tsl = TSL(self.alphabet, self.grammar, self.k, self.data,
                self.edges, polar=polarity)
            if not tsl.alphabet:
                tsl.extract_alphabet()
            tsl.tier = list(alpha)
            tsl.grammar = list(ngrams)
            tsl.fsmize()
            restr_to_fsm.append([tsl.tier[:], tsl.grammar[:], tsl.fsm])

        return restr_to_fsm


    def fsmize(self):
        """
        Builds FSM family corresponding to the given grammar and 
        saves in it the fsm attribute.
        """
        restr_to_fsm = self.map_restrictions_to_fsms()
        self.fsm.family = [i[2] for i in restr_to_fsm]


    def generate_sample(self, n=10, repeat=True, safe=True):
        """
        Generates a data sample of the required size, with or without
        repetitions depending on `repeat` value.
        Arguments:
            n (int): the number of examples to be generated;
            repeat (bool): allows (rep=True) or prohibits (rep=False)
               repetitions within the list of generated items;
            safe (bool): automatically breaks out of infinite loops,
                for example, when the grammar cannot generate the
                required number of data items, and the repetitions
                are set to False.
        Returns:
            list: generated data sample.
        """
        if not self.alphabet:
            raise ValueError("Alphabet cannot be empty.")
        if not self.fsm.family:
            self.fsmize()

        if not self.tier_smap:
            self.tier_smap = self.tier_state_maps()

        if not any([len(self.tier_smap[x]) for x in self.tier_smap]):
            raise(ValueError("There are ngrams in the grammar that are"
                            " not leading anywhere. Clean the grammar "
                            " or run `grammar.clean_grammar()`."))

        if not self.main_smap:
            self.main_smap = self.general_state_map(self.tier_smap)

        data = [self.generate_item(self.tier_smap, self.main_smap) for i in range(n)]

        if not repeat:
            data = set(data)
            useless_loops = 0
            prev_len = len(data)

            while len(data) < n:
                data.add(self.generate_item(tier_smap))

                if prev_len == len(data):
                    useless_loops += 1
                else:
                    useless_loops = 0
                
                if safe and useless_loops > 500:
                    print("The grammar cannot produce the requested "
                          "number of strings. Check the grammar, "
                          "reduce the number, or allow repetitions.")
                    break

        return list(data)


    def tier_image(self, string):
        """
        Creates tier images of a string with respect to the different
        tiers listed in the grammar.
        Returns:
            dict: a dictionary of the following shape:
                { (tier_1):"string_image_given_tier_1",
                    ...,
                  (tier_n):"string_image_given_tier_n"
                }
        """
        tiers = {}
        for i in self.grammar:
            curr_tier = ""
            for s in string:
                if s in self.edges or s in i:
                    curr_tier += s
            tiers[i] = curr_tier
        return tiers



    def generate_item(self, tier_smap, main_smap):
        """
        Generates a well-formed string with respect to the given grammar.
        Returns:
            str: a well-formed string.
        """
        word = self.edges[0] * (self.k - 1)
        tier_images = self.tier_image(word)

        while word[-1] != self.edges[1]:
            maybe = choice(main_smap[word[-(self.k - 1):]])
            good = True
            for tier in tier_smap:
                if maybe in tier:
                    old_image = tier_images[tier]
                    if maybe not in tier_smap[tier][old_image[-(self.k - 1):]]:
                        good = False
            if good:
                word += maybe
                tier_images = self.tier_image(word)

        
        newword = word[(self.k - 1):-1] 
        if self.scan(newword):    
            return newword
        else:
            return self.generate_item(tier_smap, main_smap)


    def tier_state_maps(self):
        """
        Generates a dictionary of transitions within the FSMs
        that correspond to the tier grammars.
        Returns:
            dict: the dictionary of the form
                {
                 (tier_1):{"keys":[list of next symbols]},
                 (tier_2):{"keys":[list of next symbols]},
                   ...
                 (tier_n):{"keys":[list of next symbols]},
                }, where keys are (k-1)-long tier representations.
        Warning: the list of next symbols is tier-specific,
            so this estimates the rough options: refer to
            generate_item for the filtering of wrongly
            generated items.
        """
        restr_to_fsm = self.map_restrictions_to_fsms()
        tier_smaps = {}

        for curr_tier in restr_to_fsm:
            sl = SL()
            sl.change_polarity(self.check_polarity())
            sl.edges = self.edges
            sl.k = self.k
            sl.alphabet = curr_tier[0]
            sl.grammar = curr_tier[1]
            sl.fsm = curr_tier[2]
            tier_smaps[tuple(sl.alphabet)] = sl.state_map()

        return tier_smaps


    def general_state_map(self, smaps):
        """
        Generates a dictionary of transitions within all
        FSMs of the FSM family.
        Returns:
            dict: the dictionary of the form
                {"keys":[list of next symbols]}, where 
                keys are (k-1)-long strings.
        Warning: the list of next symbols is tier-specific,
            so this estimates the rough options: refer to
            generate_item for the filtering of wrongly
            generated items.
        """
        local_smaps = deepcopy(smaps)

        for tier in local_smaps:
            non_tier = [i for i in self.alphabet if i not in tier]
            for entry in local_smaps[tier]:
                local_smaps[tier][entry].extend(non_tier)

        local_smaps = list(local_smaps.values())
        main_smap = deepcopy(local_smaps[0])

        for other in local_smaps[1:]:
            for entry in other:

                if entry not in main_smap:
                    main_smap[entry] = other[entry]
                else:
                    inter = [i for i in main_smap[entry] if i in other[entry]]
                    main_smap[entry] = inter

        free_ones = []
        for i in self.alphabet:
            for j in self.grammar:
                if i in j:
                    break
            free_ones.append(i)

        ext_alphabet = deepcopy(self.alphabet) + [self.edges[1]]
        for x in free_ones:
            main_smap[x] = ext_alphabet

        return main_smap


    def clean_grammar(self):
        """
        Removes useless ngrams from the grammar.
        If negative, it just removes duplicates.
        If positive, it detects ngrams to which one cannot get
            from the initial symbol and from which one cannot get
            to the final symbol, and removes them.
        """
        for tier in self.grammar:
            sl = SL()
            sl.change_polarity(self.check_polarity())
            sl.edges = self.edges
            sl.alphabet = list(tier)
            sl.k = self.k
            sl.grammar = self.grammar[tier]
            sl.fsmize()
            sl.clean_grammar()
            self.grammar[tier] = deepcopy(sl.grammar)


def ostia(S, Sigma, Gamma):
    """
    This function implements OSTIA (Onward Subsequential Transduction
    Inference Algorithm).
    Arguments:
        S (list): a list of pairs (o, t), where `o` is the original
            string, and `t` is its translation;
        Sigma (list): the input alphabet;
        Gamma (list): the output alphabet.
    Returns:
        FST: a transducer defining the mapping.
    """
    # create a template of the onward PTT
    T = build_ptt(S, Sigma, Gamma)
    T = onward_ptt(T, "", "")[0]
    
    # color the nodes
    red = [""]
    blue = [tr[3] for tr in T.E if tr[0] == "" and len(tr[1]) == 1]
    
    # choose a blue state
    while len(blue) != 0:
        blue_state = blue[0]

        # if exists state that we can merge with, do it
        exists = False
        for red_state in red:
            
            # if you already merged that blue state with something, stop
            if exists == True: break
                
            # try to merge these two states
            if ostia_merge(T, red_state, blue_state):
                T = ostia_merge(T, red_state, blue_state)
                exists = True
        
        # if it is not possible, color that blue state red
        if not exists:
            red.append(blue_state)
            
        # if possible, remove the folded state from the list of states
        else:
            T.Q.remove(blue_state)
            del T.stout[blue_state]
            
        # add in blue list other states accessible from the red ones that are not red
        blue = []
        for tr in T.E:
            if tr[0] in red and tr[3] not in red:
                blue.append(tr[3])
    
    # clean the transducer from non-reachable states
    T = ostia_clean(T)
                
    return T


def build_ptt(S, Sigma, Gamma):
    """
    Builds a prefix tree transducer based on the data sample.
    Arguments:
        S (list): a list of pairs (o, t), where `o` is the original
            string, and `t` is its translation;
        Sigma (list): the input alphabet;
        Gamma (list): the output alphabet.
    """
    
    # build a template for the transducer
    T = FST(Sigma, Gamma)
    
    # fill in the states of the transducer
    T.Q = []
    for i in S:
        for j in prefix(i[0]):
            if j not in T.Q:
                T.Q.append(j)
                
    # fill in the empty transitions
    T.E = []
    for i in T.Q:
        if len(i) >= 1:
            T.E.append([i[:-1], i[-1], "", i])
            
    # fill in state outputs
    T.stout = {}
    for i in T.Q:
        for j in S:
            if i == j[0]:
                T.stout[i] = j[1]
        if i not in T.stout:
            T.stout[i] = "*"
    
    return T


def onward_ptt(T, q, u):
    """
    Function recursively pushing the common parts
    of strings towards the initial state therefore
    making the machine onward.
    Arguments:
        T (FST): a transducer that is being modified;
        q (str): a state that is being processes;
        u (str): a current part of the string to be moved.
    Returns:
        (FST, str, str)
            FST: the updated transducer;
            str: a new state;
            u: a new string to be moved.
    """
    # proceed as deep as possible
    for tr in T.E:
        if tr[0] == q:
            T, qx, w = onward_ptt(T, tr[3], tr[1])
            if tr[2] != "*":
                tr[2] += w
                  
    # find lcp of all ways of leaving state 1 or stopping in it
    t = [tr[2] for tr in T.E if tr[0] == q]
    f = lcp(T.stout[q], *t)
    
    # remove from the prefix unless it's the initial state
    if f != "" and q != "":
        for tr in T.E:
            if tr[0] == q:
                tr[2] = remove_from_prefix(tr[2], f)
        T.stout[q] = remove_from_prefix(T.stout[q], f)
                
    return T, q, f


def ostia_outputs(w1,w2):
    """
    Function implementing a special comparison operation:
    it returns a string if two strings are the same and if
    another string is unknown, and False otherwise.
    Arguments:
        w1 (str): the first string;
        w2 (str): the second string.
    Returns:
        bool | if strings are not the same;
        str | otherwise.
    """
    if w1 == "*":
        return w2
    elif w2 == "*":
        return w1
    elif w1 == w2:
        return w2
    else:
        return False


def ostia_pushback(T_orig, q1, q2, a):
    """
    Re-distributes lcp of two states further in the FST.
    Arguments:
        T_orig (FST): a transducer;
        q1 (str): the first state;
        q2 (str): the second state;
        a (str): the lcp of q1 and q2.
    Returns:
        FST: an updated transducer.
    """
    # to avoid rewriting the original transducer
    T = T_orig.copy_fst()
    
    # states where you get if follow a
    q1_goes_to = None
    q2_goes_to = None
    
    # what is being written from this state
    from_q1, from_2 = None, None
    for tr in T.E:
        if tr[0] == q1 and tr[1] == a:
            from_q1 = tr[2]
            q1_goes_to = tr[3]
        if tr[0] == q2 and tr[1] == a:
            from_q2 = tr[2]
            q2_goes_to = tr[3]
    if from_q1 == None or from_q2 == None:
        raise ValueError("One of the states cannot be found.")
    
    # find the part after longest common prefix
    u = lcp(from_q1, from_q2)
    remains_q1 = from_q1[len(u):]
    remains_q2 = from_q2[len(u):]
    
    # assign lcp as current output
    for tr in T.E:
        if tr[0] in [q1, q2] and tr[1] == a:
            tr[2] = u
            
    # find what the next state writes given any other choice
    # and append the common part in it
    for tr in T.E:
        if tr[0] == q1_goes_to:
            tr[2] = remains_q1 + tr[2]
        if tr[0] == q2_goes_to:
            tr[2] = remains_q2 + tr[2]
    
    # append common part to the next state's state output
    if T.stout[q1_goes_to] != "*":
        T.stout[q1_goes_to] = remains_q1 + T.stout[q1_goes_to]
    if T.stout[q2_goes_to] != "*":
        T.stout[q2_goes_to] = remains_q2 + T.stout[q2_goes_to]
    
    return T




def ostia_merge(T_orig, q1, q2):
    """
    Re-directs all branches of q2 into q1.
    Arguments:
        T_orig (FST): a transducer;
        q1 (str): the first state;
        q2 (str): the second state.
    Returns:
        FST: an updated transducer.
    """
    # to avoid rewriting the original transducer
    T = T_orig.copy_fst()
    
    # save which transition was changed to revert in case cannot merge the states
    changed = None
    for tr in T.E:
        if tr[3] == q2:
            changed = tr[:]
            tr[3] = q1
            
    # save the state output of the q1 originally
    changed_stout = T.stout[q1]
            
    # check if we can merge the states
    can_do = ostia_fold(T, q1, q2)
    
    # if cannot, revert the change
    if can_do == False:
        for tr in T.E:
            if tr[0] == changed[0] and tr[1] == changed[1] and tr[2] == changed[2]:
                tr[3] = changed[3]
        T.stout[q1] = changed_stout
        return False
    
    # if can, do it
    else:
        return can_do




def ostia_fold(T_orig, q1, q2):
    """
    Recursively folds subtrees of q2 into q1.
    Arguments:
        T_orig (FST): a transducer;
        q1 (str): the first state;
        q2 (str): the second state.
    Returns:
        FST: an updated transducer.
    """
    # to avoid rewriting the original transducer
    T = T_orig.copy_fst()
    
    # compare the state outputs
    w = ostia_outputs(T.stout[q1], T.stout[q2])
    if w == False:
        return False
    
    # rewrite * in case it's the output of q1
    T.stout[q1] = w

    # look at every possible subtree of q_2
    for a in T.Sigma:
        add_new = False

        for tr_2 in T.E:
            if tr_2[0] == q2 and tr_2[1] == a:
                
                # if the edge exists from q1
                edge_defined = False
                for tr_1 in T.E:
                    if tr_1[0] == q1 and tr_1[1] == a:
                        edge_defined = True
                        
                        # fail if inconsistent with output of q2
                        if tr_1[2] not in prefix(tr_2[2]):
                            return False
                        
                        # move the mismatched suffix of q1 and q2 further
                        T = ostia_pushback(T, q1, q2, a)
                        T = ostia_fold(T, tr_1[3], tr_2[3])
                        if T == False: return False
                        
                # if the edge doesn't exist from q1 yet, add it
                if not edge_defined:
                    add_new = [q1, a, tr_2[2], tr_2[3]]
        
        # if the new transition was constructed, add it to the list of transitions
        if add_new:
            T.E.append(add_new)
    
    return T




def ostia_clean(T_orig):
    """
    Removes the disconnected branches from the transducer
    that appear due to the step folding the sub-trees.
    Arguments:
        T_orig (FST): a transducer.
    Returns:
        FST: an updated transducer.
    """
    # to avoid rewriting the original transducer
    T = T_orig.copy_fst()
    
    # determine which states are reachable, i.e. accessible from the initial state
    reachable_states = [""]
    add = []
    change_made = True
    while change_made == True:
        change_made = False
        for st in reachable_states:
            for tr in T.E:
                if tr[0] == st and tr[3] not in reachable_states and tr[3] not in add:
                    add.append(tr[3])
                    change_made = True

        # break out of the loop if after checking the list once again, no states were added
        if change_made == False:
            break
        else:
            reachable_states.extend(add)
            add = []
            
    # clean the list of transitions
    new_E = []
    for tr in T.E:
        if tr[0] in reachable_states and tr[3] in reachable_states:
            new_E.append(tr)
    T.E = new_E

    # clean the dictionary of state outputs
    new_stout = {}
    for i in T.stout:
        if i in reachable_states:
            new_stout[i] = T.stout[i]
    T.stout = new_stout

    # clean the list of states
    new_Q = [i for i in T.Q if i in reachable_states]
    T.Q = new_Q
    
    return T


def _keep_alive(x, memo):
    """Keeps a reference to the object x in the memo.
    Because we remember objects by their id, we have
    to assure that possibly temporary objects are kept
    alive by referencing them.
    We store a reference at the id of the memo, which should
    normally not be used unless someone tries to deepcopy
    the memo itself...
    """
    try:
        memo[id(memo)].append(x)
    except KeyError:
        # aha, this is the first one :-)
        memo[id(memo)]=[x]

class MTSL_SL:
    def __init__(self):
        self.mtsl = MTSL(k=2, polar='n')
        self.sl = SL(k=3, polar='n')
        self.data = []
        self.alphabet = []
        self.grammar = {}

    def extract_alphabet(self):
        self.mtsl.data = self.data
        self.sl.data = self.data
        self.mtsl.extract_alphabet()
        self.sl.extract_alphabet()
        self.alphabet = self.mtsl.alphabet

    def learn(self):
        self.mtsl.learn()
        self.sl.learn()
        self.grammar = dict(self.mtsl.grammar)
        self.grammar['3-SL'] = self.sl.grammar

    def scan(self, word):
        return self.sl.scan(word) and self.mtsl.scan(word)

    def generate_sample(self):
        return [word for word in self.sl.generate_sample() if self.mtsl.scan(word)]


constructors = {
    '2-MTSL': lambda: MTSL(k=2, polar='n'),
    '2-TSL':  lambda: TSL(k=2, polar='n'),
    '2-SP':   lambda: SP(k=2, polar='n'),
    '2-SL':   lambda: SL(k=2, polar='n'),
    '3-TSL':  lambda: TSL(k=3, polar='n'),
    '3-SP':   lambda: SP(k=3, polar='n'),
    '3-SL':   lambda: SL(k=3, polar='n'),
    '4-TSL':  lambda: TSL(k=4, polar='n'),
    '4-SP':   lambda: SP(k=4, polar='n'),
    '5-SL':   lambda: SL(k=4, polar='n'),

    '2-MTSL & 3-SL': lambda: MTSL_SL()
}

def handle_infer_button(event):
    words = document.select('#input textarea')[0].value.split()
    syntax_mode = document.select('#syntax-mode')[0].checked
    syntax_mode_table = {}
    next_substitute = 'A'
    if syntax_mode:
        words = []
        for line in document.select('#input textarea')[0].value.split('\n'):
            sentence_code = ''
            for word in line.split():
                if word not in syntax_mode_table:
                    syntax_mode_table[word] = next_substitute
                    next_substitute = chr(ord(next_substitute) + 1)
                    while next_substitute in '<>':
                        next_substitute = chr(ord(next_substitute) + 1)
                sentence_code += syntax_mode_table[word]
            if sentence_code:
                words.append(sentence_code)
        T = {v: k + ' ' for (k, v) in syntax_mode_table.items()}
    else:
        T = {c: c for c in set(''.join(words))}
    T['<'] = '⋉'
    T['>'] = '⋊'
    model_class = document.select('#input select')[0].value
    model = constructors[model_class]()
    model.data = words
    model.extract_alphabet()
    model.learn()
    table = html.TABLE()
    tbody = html.TBODY()
    if 'TSL' in model_class:
        tbody <= html.TR([html.TH('Tier'), html.TH('Constraint')])
        if 'MTSL' in model_class:
            for tier, constraints in model.grammar.items():
                tbody <= html.TR([html.TD('{' + ', '.join(sorted(T[c].strip() for c in tier)) + '}' if tier != '3-SL' else tier), html.TD(', '.join('*' + ''.join(T[c] for c in constraint).strip() for constraint in constraints))])
        else:
            for constraint in model.grammar:
                tbody <= html.TR([html.TD('{' + ', '.join(sorted(T[c].strip() for c in model.tier)) + '}'), html.TD('*' + ''.join(T[c] for c in constraint).strip())])
    else:
        tbody <= html.TR([html.TH('Constraint')])
        for constraint in model.grammar:
            tbody <= html.TR(html.TD('*' + ''.join(T[c] for c in constraint).strip()))

    table <= tbody
    out = document.select('#output')[0]
    out.clear()
    out <= html.H2(model_class + ' grammar')

    out <= table
    example_box = html.DIV()
    out <= example_box
    def generate_examples(event):
        example_box <= html.P((',\n' if syntax_mode else ', ').join(''.join(T[c] for c in s).strip() for s in model.generate_sample()))
    def test_membership(event):
        f = document.select('#membership-field')[0]
        string = f.value
        if syntax_mode:
            string = [syntax_mode_table[w] if w in syntax_mode_table else None for w in string.split()]
        r = document.select('#membership-result')[0]
        m = 'Yes' if all(c in model.alphabet for c in string) and model.scan(string) else 'No'
        f.class_name = m
        r.class_name = m
        r.clear()
        r <= m
    button = html.BUTTON('Generate examples')
    button.bind('click', generate_examples)
    out <= html.P(button)
    membership_field = html.INPUT(id='membership-field')
    membership_field.bind('keyup', test_membership)
    out <= html.P(['Is the string ', membership_field, ' a member of the stringset? ', html.SPAN(id='membership-result')])

def handle_lowercase_button(event):
    words_box = document.select('#input textarea')[0]
    words_box.value = words_box.value.lower()

def is_punctuation(c):
    if c in '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~':
        # string.punctuation
        return True
    if 0x2000 <= ord(c) <= 0x206F:
        # General Punctuation (Unicode)
        return True
    if 0x2E00 <= ord(c) <= 0x2E7F:
        # Supplemental Punctuation (Unicode)
        return True
    return False

def handle_punctuation_removal_button(event):
    words_box = document.select('#input textarea')[0]
    words_box.value = ''.join((c if not is_punctuation(c) else ' ') for c in words_box.value)

document.select('#infer')[0].bind('click', handle_infer_button)
document.select('#lowercase')[0].bind('click', handle_lowercase_button)
document.select('#remove-punctuation')[0].bind('click', handle_punctuation_removal_button)
    
</script>


</body>

</html>
